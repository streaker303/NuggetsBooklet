我们经常会问 AI 一些问题，大多数情况下，它会综合各种资料，给我们想要的回答。

但有的时候就不行了：

比如我写了 10 年的日记，这些都是私密信息，没有公开。

那让 AI 给我总结下我的一些经历，是不是就做不到了？

那如何在不把这些信息公开到互联网的情况下，让 AI 给我总结下呢？

这时候我们就需要搭建本地的知识库了。

而且搭了本地知识库也不行，用线上的 AI 模型的时候，万一它把我的日记记下来了呢？

如果我想绝对的安全那就也不能用这些线上模型。

这时候就需要跑本地 AI 模型了。

所以，这篇文章我们就来学习下本地知识库 + 本地模型。

除了上面说的安全外，它们还不用花一分钱！

首先，我们先用下线上的 AI 模型。

国内用国外的模型一般都要找个代理商来用。

这种代理挺多的，我这里用的 [302.ai](https://gpt302.saaslink.net/bzgo6g "https://gpt302.saaslink.net/bzgo6g")，你也可以用别的

这种代理都是给你对接好了国内、国外各种模型，可以直接用。

充值后，生成一个 API KEY：

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b0f21134da794007971d552e35e8bdcd~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2580&h=1340&s=274617&e=png&b=ffffff)

接下来就可以调用 openai 的接口了。

我们创建个项目：

```perl
mkdir my-ai-test
cd my-ai-test
npm init -y
```

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a2b9dbe838e46bea1dec4a915e70db2~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=892&h=678&s=87470&e=png&b=010101)

进入项目，安装 openai 的 sdk：

```css
npm install --save openai
```

然后创建 index.mjs（.mjs 的后缀是告诉 node 这个文件是 es module 的）

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: '你的 API KEY',
    baseURL: 'https://api.302.ai/v1'
});

async function main() {
  const stream = await client.chat.completions.create({
    model: "gpt-4",
    messages: [
      { role: 'user', content: '今天晚上吃什么，给我一些推荐一些清淡点的菜' },
    ],
    stream: true
  });

  for await (const chunk of stream) {
    process.stdout.write(chunk.choices[0]?.delta?.content || '');
  }
}

main();
```

跑一下：

```bash
node ./src/index.mjs
```

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/25031421eae94a1c976667ffe370d93b~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.gif#?w=1864&h=998&s=155094&e=gif&f=59&b=1a1a1a)

如果问它我的日记里的一些东西，他就不知道了：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae4fd839c5dd47a4b8e06ce8ce270f07~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2136&h=652&s=158496&e=png&b=1d1d1d)

这时候就需要搭建一个知识库了。

我这里用的是 [maxkb](https://maxkb.cn/ "https://maxkb.cn/")，一个开源的知识库

用 docker 跑一下：

首先没安装 docker 的话去官网下载下桌面端 [www.docker.com/](https://www.docker.com/ "https://www.docker.com/")

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6478ee495cc04028a5ce4586f4364842~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2264&h=1052&s=180680&e=png&b=ffffff)

跑起来后，可以看到镜像列表：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1db2ce29f879468488c11da3d53b09f2~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2096&h=1178&s=270255&e=png&b=f6f6f8)

镜像跑起来的容器列表：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8f3a30432c448d68f7c4cd21ab1ca77~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2106&h=1078&s=295037&e=png&b=f6f6f8)

我们搜索下 maxkb 的镜像（这步可能需要科学上网）：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/635687b987ae418b8628c5a311ef05f3~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1846&h=1132&s=227729&e=png&b=f2f3f7)

点击 run，它会下载镜像，然后让你填入参数来跑：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd14b097cf3547448610ae2a54d874ee~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1200&h=1308&s=166106&e=png&b=fefefe)

输入容器名、映射的端口号、挂载的数据卷

映射端口的意思是你把本地的 8080 端口映射到容器里的 8080 端口，这样你就能通过本地的 8080 端口访问容器里的服务了。

挂载数据卷也是这个意思，把本地的 /Users/guang/.maxkb（这个目录是随便的，windows 下就是 c://xxx ） 目录挂载到容器里的 /var/lib/postgresql/data（这个目录是固定的）

还需要挂载本地的另一个目录到容器里的 /opt/maxkb/app/sandbox/python-packages

接下来点击 run，把容器跑起来：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e5d7c96813b94bee828c8aca091fec11~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1648&h=1180&s=276688&e=png&b=ffffff)

跑起来之后访问 [http://localhost:8080](http://localhost:8080 "http://localhost:8080") 就好了。

登录页面是这样的：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/47a3f19269a24adcaad68b7d62d46edb~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2376&h=1256&s=2004435&e=png&b=ffffff)

初始用户名和密码是： admin、MaxKB@123..

点击创建知识库：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0d9066fb18f749708a8b3ee9d8e59ee5~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1402&h=804&s=110869&e=png&b=f3f5f8)

这个知识库是放我的看过的小说：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/197a5d982c8f46738b7e29f6cbaa36e2~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1432&h=1102&s=113579&e=png&b=ffffff)

点击上传文档：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0030563c13fb4fe8afa469603bf2bd74~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1696&h=602&s=85268&e=png&b=ffffff)

文档可以是文本的 txt、markdown 等，也可以是表格的 excel、csv 等：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bf2e4119a9cb4f019a65443bfe0e7e73~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1582&h=828&s=99770&e=png&b=ffffff)

把这个小说上传上去

秋日的秘密.txt

```txt
秋天的午后，阳光透过金黄的树叶洒在小镇的石板路上，给人一种温暖而宁静的感觉。艾米莉提着一篮苹果，沿着小路走向镇外的森林。她的步伐轻快，心中充满了期待，因为她正准备去见一个不为人知的秘密朋友。

在森林的深处，有一片隐蔽的空地，那里有一座古老的石亭，几乎被藤蔓和苔藓覆盖。艾米莉第一次发现这个地方是在一个月前，她偶然间走得太远，结果误入了这片神秘的区域。自那以后，她每周都会来这里，和她的新朋友见面。

她的朋友是一个名叫卢卡斯的年轻画家，他有着一头卷曲的黑发和一双总是带着微笑的眼睛。卢卡斯在镇上并不为人所知，因为他总是躲在森林里，专注于他的画作。他的画布上充满了色彩斑斓的秋景，每一幅都像是一个梦境。

“艾米莉！”卢卡斯的声音从亭子里传来，他正站在一幅未完成的画作前，手中握着调色板。

“我带了些苹果来，”艾米莉微笑着回应，将篮子放在石桌上。“你今天画了什么？”

卢卡斯指了指画布，上面是一片金色的树林，阳光透过树叶，形成斑驳的光影。“这是我今天早上看到的景象。秋天总是充满了灵感。”

艾米莉仔细端详着画，心中不由得感叹卢卡斯的才华。“如果你把这些画拿去镇上展出，一定会大受欢迎的。”

卢卡斯笑了笑，摇摇头。“我更喜欢这里的安静。我画画是为了自己，而不是为了展示。”

两人聊了一会儿，直到太阳开始西沉，空气中弥漫着一丝凉意。艾米莉知道是时候回家了，她不想让父母担心。

“明天见，卢卡斯。”她挥了挥手，准备离开。

“明天见，艾米莉。”卢卡斯微笑着回应，目送她的身影消失在树林间。

然而，第二天，当艾米莉再次来到石亭时，卢卡斯却没有出现。她等了很久，心中开始有些不安。她在森林里四处寻找，但卢卡斯仿佛从未存在过一样，消失得无影无踪。

接下来的几天，艾米莉依然没有见到卢卡斯，她开始担心是不是发生了什么事情。她决定去镇上的画廊打听，因为她记得卢卡斯曾提到过他有一个朋友在那里工作。

画廊的老板是一个和蔼的老人，他听完艾米莉的描述后，露出了一个神秘的微笑。“你说的卢卡斯，是不是有一头卷曲的黑发？”

艾米莉点点头，心中燃起一丝希望。

“那可能是我年轻时的一个朋友，”老人说道，眼中闪烁着怀旧的光芒。“他是个天才画家，但很久以前就离开了这里。听说他去了一个遥远的地方。”

艾米莉愣住了，她不知道该如何理解这些话。她开始怀疑自己是否在森林中遇见的卢卡斯只是一个幻影，或者是她自己的想象。

但当她回到石亭时，发现桌上多了一幅画，那是她和卢卡斯在一起的情景。画中的两人笑得那么真实，仿佛在告诉她，那个秋日的秘密确实存在过。

艾米莉小心翼翼地拿起画，心中充满了温暖。也许卢卡斯真的去了一个遥远的地方，但他留下的画作和回忆，将永远伴随着她。

从那以后，艾米莉每年秋天都会去石亭，带上一篮苹果，静静地坐在那儿，仿佛卢卡斯还在她身边，继续讲述着那些关于秋天的秘密。
```

这个小说绝对是网上搜不到的，因为我是让 AI 写的：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/310ec0632ca94e22b3e3b6317578589f~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1732&h=880&s=196272&e=png&b=fafafa)

不信你可以问下试试：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3f730b42c523409c9d5f41d725a0b22b~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1996&h=748&s=169456&e=png&b=1d1d1d)

上传这个文档：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0a3ec3387fc478089db53517da2e3e0~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2174&h=1176&s=141206&e=png&b=ffffff)

这样，知识库里就有了一个文档：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dab68d2a49514996b1dcb009c358cd46~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1230&h=596&s=68618&e=png&b=ffffff)

然后点击创建一个应用：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f6b1bf82a1e0493da8b1610cd287d8c5~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1390&h=676&s=103599&e=png&b=f2f5f8)

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/117e9e0b62664c718e3bd9142cc9f09b~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1444&h=1002&s=113541&e=png&b=ffffff)

配置应用前，我们先要配置下线上的模型：

点击系统管理 > 模型设置 > 添加模型：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/158c31d1a1894256a92f070de26f85c6~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2188&h=972&s=255432&e=png&b=ffffff)

添加一个 openai 的模型，填入刚才我们用的代理商的 api key 和 base url

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/777c591bd93446ab8c6369a334456527~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1840&h=1202&s=235130&e=png&b=ffffff)

点击添加：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35e641b8eea34257a8f581fefb6d381d~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1874&h=1414&s=268516&e=png&b=ffffff)

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c39640f85544cd6ab7add8e597492bf~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2194&h=744&s=142304&e=png&b=ffffff)

然后回过头来继续配置应用：

选择刚才创建的模型：

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5f9022a93e394ba3837970590925c673~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1060&h=966&s=105715&e=png&b=ffffff)

关联上小说的知识库：

![2024-11-23 20.01.55.gif](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a46db398b9ff45a79afba15dc87349ae~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.gif#?w=2696&h=1346&s=3025869&e=gif&f=48&b=fefefe)

点击右上角的保存并发布。

之后就可以在概览里看到 base url 和 api key 了：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c5278b4bbf9409e974563bb10d28cb4~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2258&h=576&s=120319&e=png&b=ffffff)

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cff6f5cf61474244ab960b38d2252b31~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1604&h=538&s=59053&e=png&b=ffffff)

把它复制出来，替换 index.mjs 里的配置：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/57a5d68adc4b41e7aaa122b230f67e3c~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1628&h=242&s=62515&e=png&b=1f1f1f)

这时候就可以问这本小说的问题了：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b54453905fca4e59bcab428d04859a09~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2112&h=798&s=307437&e=png&b=1b1b1b)

我们多问点问题试试：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/57cbc33b569a4de0ae4d149b8e3e1027~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2138&h=394&s=114686&e=png&b=1c1c1c)

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fd0a8ca4bafc48cb84023a6ab6d70b7b~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2140&h=360&s=115873&e=png&b=1c1c1c)

回答的都挺好的。

当然，这个小说内容比较少，你完全可以找一个内容更多的知识库，来让 AI 解读。

这就是知识库的作用！

我们企业内部的一些私有组件库，就可以把信息整理一下，做成知识库告诉 AI。

这样，它再生成的代码就是基于内部组件库的了。

那再看下第二个问题，虽然知识库是本地部署的，但是用的模型是线上的啊

这样可能有安全问题，所以我们要自己部署个本地模型。

我们用 ollama 来安装本地模型。

在 ollama 官网 [ollama.com/](https://ollama.com/ "https://ollama.com/") 下载：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fc605d1b8d4e4595a0aa8aa044e1e68c~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2346&h=1232&s=155010&e=png&b=ffffff)

安装后就可以用 ollama 命令了。

我们跑下 qwen 这个模型，它是中文开源模型：

```arduino
ollama run qwen2.5:1.5b
```

从 0.5b 到 72b 是参数多少不同，消耗计算资源不同：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/27ec252628db446fbaa593bb76cf1f52~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2002&h=1196&s=174876&e=png&b=fefefe)

我们这里下个小的来测试：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a80beed3c4e24192b8a56b70805f7a10~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1376&h=808&s=191801&e=png&b=010101)

下载后问了下问题，可以看到，大模型跑起来了。

这个窗口不要关掉，再开一个窗口调用下 api：

```vbnet
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:1.5b",
  "messages": [
    { "role": "user", "content": "介绍下西游记" }
  ]
}'
```

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d8d8f272050646e18eabc141c8e28441~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1154&h=744&s=165483&e=png&b=020202)

上面是流式返回的内容。

可以看到 API 也是通的。

接下来就可以把这个本地模型配到 maxkb 了。

添加 Ollama 模型：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/06a4babf748d4283acdbbeecdc2322fa~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2010&h=1060&s=193442&e=png&b=ffffff)

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d95feaa4d18741d68fde060d0db18112~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1322&h=1402&s=170200&e=png&b=ffffff)

填下名字、模型名、API 域名和 API Key

这里模型名填和本地一样的

API 域名用 [host.docker.internal:11434](http://host.docker.internal:11434 "http://host.docker.internal:11434") （就是容器内的 locahost）

API Key 填啥都行

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4366773ba19f41cca7fd9edbd2e2fe39~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2106&h=892&s=130762&e=png&b=ffffff)

然后改下小说助手的设置，换成本地的 Ollama 模型，点击保存：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d4c9c3cda8294d958acb00442e56021f~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1498&h=802&s=89383&e=png&b=ffffff)

为了证明我这次用的是本地的模型，我把线上 AI 的所有 API Key 都禁用掉了：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1f23f0d0b02a44718d88b6e6a67fa816~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=2742&h=1172&s=263772&e=png&b=ffffff)

然后我让它给续写一段故事：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f33226ec9f3b44aa8e9d8d364b2e8d38~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1752&h=1150&s=398199&e=png&b=1a1a1a)

你仔细看下，写的还真挺好的。

本地用的这个 qwen2.5 模型真不错，一般情况下不用花钱去买线上的模型用了。

用本地知识库放一些私有的信息，然后交给本地的 AI 模型去解读，完全不用花一分钱，效果也足够用。

## 总结

今天我们学了本地知识库、本地 AI 模型，并用它来做了一个阅读小说的助手。

AI 模型只知道公开的信息，如果想让它对一些你的私有信息做解读，就需要用到知识库了。

我们用 docker 跑了 maxkb 这个开源知识库，完全免费用，数据都是存在本地的相当安全。

你可以用它接入线上 AI 模型来用，但如果担心这样不安全，也可以用 Ollma 跑个本地 AI 模型。

我们试了下本地 Ollama 跑的 qwen2.5 模型，然后搭配小说知识库里的一篇小说，续写啥的完全不成问题，解读也很到位。

当然，这些不是前端领域的应用。

对前端来说，最有价值的是把内部的组件库、文档等做成知识库，让 AI 生成的代码用内部组件库来写，也可以让它来做文档查询助手。

本地的 AI 模型 + 本地的知识库，完全不用花一分钱，但效果也足够用。